Our implementation involves fine-tuning a pre-trained Transformer-based language model on a standard benchmark dataset for Named Entity Recognition. The core technical work  centered on the data preprocessing pipeline, where we implement a standard methodology to align the dataset's word-level labels with the model's subword tokenization scheme. The model  then be trained following a conventional fine-tuning procedure using hyperparameters widely established in the literature for this architecture. Finally, the model's performance  evaluated using established metrics for sequence labeling tasks, allowing for a direct comparison against published baseline results.
