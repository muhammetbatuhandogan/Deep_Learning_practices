{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b7b3288-517e-4de5-ae1d-66a17c93cc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from local files...\n",
      "Dataset Loaded Successfully!\n",
      "Number of Nodes: 2708\n",
      "Feature Dimension: 1433\n",
      "Number of Classes: 7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DATA_DIR = \"./data/cora\"\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    \"\"\"Encodes class labels (strings) into one-hot vectors.\"\"\"\n",
    "    classes = sorted(list(set(labels)))\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "    return labels_onehot, classes\n",
    "\n",
    "def load_data(path=\"./data/cora\"):\n",
    "    \"\"\"\n",
    "    Loads the manually downloaded Cora dataset into PyTorch tensors.\n",
    "    \n",
    "    Expected files in 'path':\n",
    "    - cora.content\n",
    "    - cora.cites\n",
    "    \"\"\"\n",
    "    print(\"Loading dataset from local files...\")\n",
    "\n",
    "    content_path = os.path.join(path, \"cora.content\")\n",
    "    cites_path = os.path.join(path, \"cora.cites\")\n",
    "\n",
    "    # Check if files exist\n",
    "    if not os.path.exists(content_path) or not os.path.exists(cites_path):\n",
    "        raise FileNotFoundError(f\"Error: Files not found in {path}. Please download cora.content and cora.cites manually.\")\n",
    "\n",
    "    # 1. Read Content (Features & Labels)\n",
    "    # Format: <paper_id> <word_attributes>+ <class_label>\n",
    "    try:\n",
    "        idx_features_labels = np.genfromtxt(content_path, dtype=np.dtype(str))\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing cora.content. Ensure it is a clean text file, not HTML.\")\n",
    "        raise e\n",
    "    \n",
    "    # Extract features (Store as sparse matrix first)\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    \n",
    "    # Extract labels\n",
    "    labels_raw = idx_features_labels[:, -1]\n",
    "    labels_onehot, class_names = encode_onehot(labels_raw)\n",
    "    \n",
    "    # Map Paper IDs to 0-based indices\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    \n",
    "    # 2. Read Graph Structure (Edges)\n",
    "    # Format: <cited paper ID> <citing paper ID>\n",
    "    edges_unordered = np.genfromtxt(cites_path, dtype=np.int32)\n",
    "    \n",
    "    # Convert IDs to our 0..N indices\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    \n",
    "    # Build Adjacency Matrix\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels_onehot.shape[0], labels_onehot.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # Symmetrize the graph (A->B implies B->A)\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    \n",
    "    # Convert to PyTorch Tensors\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels_onehot)[1])\n",
    "    \n",
    "    # Convert adjacency matrix to dense tensor\n",
    "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "\n",
    "    # 3. Train / Val / Test Split\n",
    "    idx_train = torch.LongTensor(range(140))\n",
    "    idx_val = torch.LongTensor(range(200, 500))\n",
    "    idx_test = torch.LongTensor(range(500, 1500))\n",
    "\n",
    "    print(f\"Dataset Loaded Successfully!\")\n",
    "    print(f\"Number of Nodes: {features.shape[0]}\")\n",
    "    print(f\"Feature Dimension: {features.shape[1]}\")\n",
    "    print(f\"Number of Classes: {len(class_names)}\")\n",
    "    \n",
    "    return features, adj, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "# --- EXECUTE ---\n",
    "features, adj, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc8b60b-8f9e-4168-8c63-e5dd3db0eb1c",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "\n",
    "In this section, we loaded the **Cora** citation network dataset. \n",
    "- **Preprocessing:** We normalized the graph structure by symmetrizing the adjacency matrix (treating citations as undirected edges).\n",
    "- **Format:** The data is converted into PyTorch tensors to be compatible with the model.\n",
    "- **Data Statistics:**\n",
    "  - **Nodes:** 2708 (Scientific papers)\n",
    "  - **Edges:** Citation links between papers\n",
    "  - **Features:** 1433 (Bag-of-words representation for each paper)\n",
    "  - **Classes:** 7 (Subject categories of papers)\n",
    "  - **Split:** Standard split (140 Train, 500 Val, 1000 Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e828ace7-4a17-48cc-9417-c926ab8b0f68",
   "metadata": {},
   "source": [
    "# paper implementation part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb914df9-51f7-4ca9-8d00-0bda7fdb8bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphAttentionLayer class defined successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout        # Dropout probability\n",
    "        self.in_features = in_features # Input feature dimension\n",
    "        self.out_features = out_features # Output feature dimension\n",
    "        self.alpha = alpha            # LeakyReLU negative slope\n",
    "        self.concat = concat          # True for all layers except the output layer\n",
    "\n",
    "        # Xavier Initialization for weights (as per the paper)\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        \n",
    "        # Attention Mechanism Learnable Parameters (a vector)\n",
    "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        \"\"\"\n",
    "        h:   Input features (N, in_features)\n",
    "        adj: Adjacency matrix (N, N)\n",
    "        \"\"\"\n",
    "        # Linear Transformation (Equation 1)\n",
    "        # Wh: (N, out_features)\n",
    "        Wh = torch.mm(h, self.W) \n",
    "        \n",
    "        # --- Attention Mechanism ---\n",
    "        # We need to compute attention scores e_ij for all pairs.\n",
    "        # Paper Eq 3: e_ij = LeakyReLU(a^T * [Wh_i || Wh_j])\n",
    "        \n",
    "        # Implementation trick to avoid loops:\n",
    "        # a_input is a preparation to broadcast the addition later.\n",
    "        # We calculate (a^T * Wh_i) + (a^T * Wh_j) which is equivalent to a^T * [Wh_i || Wh_j]\n",
    "        \n",
    "        # a1: Learnable vector for the first part of concatenation\n",
    "        a_input = self._prepare_attentional_mechanism_input(Wh)\n",
    "        \n",
    "        # e: Attention scores (N, N)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "\n",
    "        # --- Masking ---\n",
    "        # We only want to attend to neighbors. \n",
    "        # The adj matrix has 1 for neighbors, 0 otherwise.\n",
    "        # Where adj is 0, we set attention score to -1e9 (very small number).\n",
    "        # When Softmax is applied, exp(-1e9) becomes 0.\n",
    "        zero_vec = -9e15 * torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        \n",
    "        # Normalize scores (Equation 3 - Softmax)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        \n",
    "        # Apply dropout to normalized attention coefficients (Regularization)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        \n",
    "        # --- Aggregation ---\n",
    "        # Equation 4: h_prime = sum(alpha_ij * Wh_j)\n",
    "        h_prime = torch.matmul(attention, Wh)\n",
    "\n",
    "        if self.concat:\n",
    "            # If this is a hidden layer, apply ELU activation (Equation 4)\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            # If this is the output layer, just return the raw values (Equation 6)\n",
    "            return h_prime\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        # Helper function to broadcast inputs for attention calculation\n",
    "        N = Wh.size()[0] # Number of nodes\n",
    "\n",
    "        # Below code creates a matrix where:\n",
    "        # matrix[i, j] contains [Wh_i, Wh_j] concatenated\n",
    "        # But we do it efficiently using broadcasting\n",
    "        \n",
    "        Wh_repeated_in_chunks = Wh.repeat_interleave(N, dim=0)\n",
    "        Wh_repeated_alternating = Wh.repeat(N, 1)\n",
    "        \n",
    "        # combination_matrix: (N * N, 2 * out_features)\n",
    "        all_combinations_matrix = torch.cat([Wh_repeated_in_chunks, Wh_repeated_alternating], dim=1)\n",
    "        \n",
    "        return all_combinations_matrix.view(N, N, 2 * self.out_features)\n",
    "\n",
    "print(\"GraphAttentionLayer class defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a72171c3-8b6a-4ed3-a987-16a91abe3bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline GAT model class defined successfully.\n"
     ]
    }
   ],
   "source": [
    "class GAT(nn.Module):\n",
    "    \"\"\"\n",
    "    The full GAT model as described in the paper.\n",
    "    Structure:\n",
    "    - Layer 1: Multi-head attention (Concatenation)\n",
    "    - Layer 2: Single-head attention (Output)\n",
    "    \"\"\"\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        \"\"\"\n",
    "        nfeat:  Number of input features (1433 for Cora)\n",
    "        nhid:   Number of hidden features per head (8 for Cora)\n",
    "        nclass: Number of output classes (7 for Cora)\n",
    "        dropout: Dropout probability (0.6 typical)\n",
    "        alpha:  LeakyReLU negative slope (0.2 typical)\n",
    "        nheads: Number of attention heads (8 typical)\n",
    "        \"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # 1. Multi-Head Attention Layer\n",
    "        # We create a list of GraphAttentionLayer modules\n",
    "        self.attentions = nn.ModuleList([\n",
    "            GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) \n",
    "            for _ in range(nheads)\n",
    "        ])\n",
    "\n",
    "        # 2. Output Layer\n",
    "        # Input size is (nheads * nhid) because we concatenated the outputs of Layer 1\n",
    "        # Output size is nclass (7)\n",
    "        # concat=False because we want to average or just return the final logits, not concat ELU\n",
    "        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        # Apply dropout to input features\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        \n",
    "        # Layer 1: Apply all attention heads and concatenate their outputs\n",
    "        # Each head returns (N, nhid), concatenated becomes (N, nheads * nhid)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        \n",
    "        # Apply dropout to hidden representation\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        \n",
    "        # Layer 2: Output attention layer\n",
    "        x = self.out_att(x, adj)\n",
    "        \n",
    "        # Log Softmax for classification (NLLLoss compatible)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "print(\"Baseline GAT model class defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de863268-f487-4f93-b683-538bae81b784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture:\n",
      "GAT(\n",
      "  (attentions): ModuleList(\n",
      "    (0-7): 8 x GraphAttentionLayer(\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "  )\n",
      "  (out_att): GraphAttentionLayer(\n",
      "    (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      ")\n",
      "\n",
      "Forward pass successful!\n",
      "Input Shape: torch.Size([2708, 1433])\n",
      "Output Shape: torch.Size([2708, 7])\n"
     ]
    }
   ],
   "source": [
    "# --- MODEL INITIALIZATION CHECK ---\n",
    "# Hyperparameters from the paper\n",
    "args_cuda = torch.cuda.is_available() \n",
    "device = torch.device(\"cuda\" if args_cuda else \"cpu\")\n",
    "\n",
    "model = GAT(nfeat=features.shape[1], \n",
    "            nhid=8, \n",
    "            nclass=labels.max().item() + 1, \n",
    "            dropout=0.6, \n",
    "            nheads=8, \n",
    "            alpha=0.2)\n",
    "\n",
    "# Move data and model to device (CPU or GPU)\n",
    "model.to(device)\n",
    "features = features.to(device)\n",
    "adj = adj.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "print(f\"Model Architecture:\\n{model}\")\n",
    "\n",
    "# Try a forward pass (Test output shape)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(features, adj)\n",
    "    print(f\"\\nForward pass successful!\")\n",
    "    print(f\"Input Shape: {features.shape}\") # (2708, 1433)\n",
    "    print(f\"Output Shape: {output.shape}\")  # (2708, 7) - Should be equal to classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "944f1d1a-eb92-4f93-9cd7-7649f880c9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch: 0001 | Loss Train: 2.2965 | Acc Train: 0.0929 | Loss Val: 1.8935 | Acc Val: 0.2667 | Time: 0.9244s\n",
      "Epoch: 0002 | Loss Train: 2.2542 | Acc Train: 0.1643 | Loss Val: 1.7961 | Acc Val: 0.4433 | Time: 0.1933s\n",
      "Epoch: 0003 | Loss Train: 1.9975 | Acc Train: 0.2357 | Loss Val: 1.7027 | Acc Val: 0.4833 | Time: 0.1957s\n",
      "Epoch: 0004 | Loss Train: 1.9224 | Acc Train: 0.2857 | Loss Val: 1.6171 | Acc Val: 0.5300 | Time: 0.2001s\n",
      "Epoch: 0005 | Loss Train: 1.7714 | Acc Train: 0.3429 | Loss Val: 1.5412 | Acc Val: 0.5667 | Time: 0.2019s\n",
      "Epoch: 0006 | Loss Train: 1.7109 | Acc Train: 0.3786 | Loss Val: 1.4751 | Acc Val: 0.5767 | Time: 0.2017s\n",
      "Epoch: 0007 | Loss Train: 1.6803 | Acc Train: 0.3857 | Loss Val: 1.4166 | Acc Val: 0.5800 | Time: 0.1969s\n",
      "Epoch: 0008 | Loss Train: 1.6166 | Acc Train: 0.4286 | Loss Val: 1.3619 | Acc Val: 0.6000 | Time: 0.1958s\n",
      "Epoch: 0009 | Loss Train: 1.5101 | Acc Train: 0.4714 | Loss Val: 1.3120 | Acc Val: 0.6267 | Time: 0.1918s\n",
      "Epoch: 0010 | Loss Train: 1.4965 | Acc Train: 0.4857 | Loss Val: 1.2660 | Acc Val: 0.6667 | Time: 0.1941s\n",
      "Epoch: 0011 | Loss Train: 1.3959 | Acc Train: 0.5429 | Loss Val: 1.2230 | Acc Val: 0.6900 | Time: 0.1940s\n",
      "Epoch: 0012 | Loss Train: 1.3031 | Acc Train: 0.5571 | Loss Val: 1.1829 | Acc Val: 0.7233 | Time: 0.1981s\n",
      "Epoch: 0013 | Loss Train: 1.2441 | Acc Train: 0.5500 | Loss Val: 1.1448 | Acc Val: 0.7267 | Time: 0.1939s\n",
      "Epoch: 0014 | Loss Train: 1.3967 | Acc Train: 0.4857 | Loss Val: 1.1092 | Acc Val: 0.7500 | Time: 0.1956s\n",
      "Epoch: 0015 | Loss Train: 1.2832 | Acc Train: 0.6071 | Loss Val: 1.0766 | Acc Val: 0.7833 | Time: 0.1938s\n",
      "Epoch: 0016 | Loss Train: 1.2282 | Acc Train: 0.5500 | Loss Val: 1.0471 | Acc Val: 0.7867 | Time: 0.1964s\n",
      "Epoch: 0017 | Loss Train: 1.2142 | Acc Train: 0.6071 | Loss Val: 1.0199 | Acc Val: 0.8000 | Time: 0.1931s\n",
      "Epoch: 0018 | Loss Train: 1.1888 | Acc Train: 0.6143 | Loss Val: 0.9937 | Acc Val: 0.8133 | Time: 0.1980s\n",
      "Epoch: 0019 | Loss Train: 1.2219 | Acc Train: 0.6143 | Loss Val: 0.9690 | Acc Val: 0.8167 | Time: 0.1953s\n",
      "Epoch: 0020 | Loss Train: 1.1122 | Acc Train: 0.6714 | Loss Val: 0.9469 | Acc Val: 0.8200 | Time: 0.1959s\n",
      "Epoch: 0021 | Loss Train: 1.0894 | Acc Train: 0.6143 | Loss Val: 0.9260 | Acc Val: 0.8200 | Time: 0.1951s\n",
      "Epoch: 0022 | Loss Train: 1.0710 | Acc Train: 0.6714 | Loss Val: 0.9072 | Acc Val: 0.8167 | Time: 0.1965s\n",
      "Epoch: 0023 | Loss Train: 0.9869 | Acc Train: 0.6929 | Loss Val: 0.8888 | Acc Val: 0.8267 | Time: 0.1951s\n",
      "Epoch: 0024 | Loss Train: 1.0721 | Acc Train: 0.6857 | Loss Val: 0.8708 | Acc Val: 0.8300 | Time: 0.1992s\n",
      "Epoch: 0025 | Loss Train: 0.9686 | Acc Train: 0.6786 | Loss Val: 0.8537 | Acc Val: 0.8300 | Time: 0.1924s\n",
      "Epoch: 0026 | Loss Train: 1.0757 | Acc Train: 0.6643 | Loss Val: 0.8380 | Acc Val: 0.8333 | Time: 0.1961s\n",
      "Epoch: 0027 | Loss Train: 0.9906 | Acc Train: 0.6714 | Loss Val: 0.8242 | Acc Val: 0.8333 | Time: 0.1953s\n",
      "Epoch: 0028 | Loss Train: 0.9786 | Acc Train: 0.6571 | Loss Val: 0.8117 | Acc Val: 0.8333 | Time: 0.1968s\n",
      "Epoch: 0029 | Loss Train: 1.0622 | Acc Train: 0.6500 | Loss Val: 0.8001 | Acc Val: 0.8333 | Time: 0.1946s\n",
      "Epoch: 0030 | Loss Train: 1.0109 | Acc Train: 0.6571 | Loss Val: 0.7894 | Acc Val: 0.8333 | Time: 0.1974s\n",
      "Epoch: 0031 | Loss Train: 0.9476 | Acc Train: 0.6857 | Loss Val: 0.7791 | Acc Val: 0.8333 | Time: 0.1930s\n",
      "Epoch: 0032 | Loss Train: 0.8550 | Acc Train: 0.7000 | Loss Val: 0.7691 | Acc Val: 0.8333 | Time: 0.1966s\n",
      "Epoch: 0033 | Loss Train: 1.0440 | Acc Train: 0.6214 | Loss Val: 0.7597 | Acc Val: 0.8400 | Time: 0.1971s\n",
      "Epoch: 0034 | Loss Train: 1.0322 | Acc Train: 0.6357 | Loss Val: 0.7503 | Acc Val: 0.8367 | Time: 0.1932s\n",
      "Epoch: 0035 | Loss Train: 0.8946 | Acc Train: 0.6786 | Loss Val: 0.7411 | Acc Val: 0.8367 | Time: 0.1949s\n",
      "Epoch: 0036 | Loss Train: 0.9840 | Acc Train: 0.6714 | Loss Val: 0.7332 | Acc Val: 0.8333 | Time: 0.1943s\n",
      "Epoch: 0037 | Loss Train: 0.9478 | Acc Train: 0.6786 | Loss Val: 0.7250 | Acc Val: 0.8367 | Time: 0.1954s\n",
      "Epoch: 0038 | Loss Train: 0.8087 | Acc Train: 0.7500 | Loss Val: 0.7169 | Acc Val: 0.8333 | Time: 0.1935s\n",
      "Epoch: 0039 | Loss Train: 0.9500 | Acc Train: 0.6571 | Loss Val: 0.7099 | Acc Val: 0.8333 | Time: 0.1951s\n",
      "Epoch: 0040 | Loss Train: 0.8845 | Acc Train: 0.6714 | Loss Val: 0.7030 | Acc Val: 0.8333 | Time: 0.1950s\n",
      "Epoch: 0041 | Loss Train: 0.8746 | Acc Train: 0.7071 | Loss Val: 0.6974 | Acc Val: 0.8333 | Time: 0.1961s\n",
      "Epoch: 0042 | Loss Train: 0.8484 | Acc Train: 0.6786 | Loss Val: 0.6918 | Acc Val: 0.8333 | Time: 0.1948s\n",
      "Epoch: 0043 | Loss Train: 0.7526 | Acc Train: 0.7357 | Loss Val: 0.6865 | Acc Val: 0.8333 | Time: 0.1972s\n",
      "Epoch: 0044 | Loss Train: 0.7811 | Acc Train: 0.7286 | Loss Val: 0.6807 | Acc Val: 0.8300 | Time: 0.1938s\n",
      "Epoch: 0045 | Loss Train: 0.8880 | Acc Train: 0.7071 | Loss Val: 0.6750 | Acc Val: 0.8300 | Time: 0.1979s\n",
      "Epoch: 0046 | Loss Train: 0.8098 | Acc Train: 0.7571 | Loss Val: 0.6702 | Acc Val: 0.8367 | Time: 0.1925s\n",
      "Epoch: 0047 | Loss Train: 0.8382 | Acc Train: 0.7071 | Loss Val: 0.6656 | Acc Val: 0.8400 | Time: 0.1971s\n",
      "Epoch: 0048 | Loss Train: 0.8463 | Acc Train: 0.7071 | Loss Val: 0.6618 | Acc Val: 0.8400 | Time: 0.1950s\n",
      "Epoch: 0049 | Loss Train: 0.7907 | Acc Train: 0.7286 | Loss Val: 0.6584 | Acc Val: 0.8400 | Time: 0.1969s\n",
      "Epoch: 0050 | Loss Train: 0.9268 | Acc Train: 0.7000 | Loss Val: 0.6556 | Acc Val: 0.8400 | Time: 0.1939s\n",
      "Epoch: 0051 | Loss Train: 0.7578 | Acc Train: 0.7429 | Loss Val: 0.6538 | Acc Val: 0.8400 | Time: 0.1963s\n",
      "Epoch: 0052 | Loss Train: 0.8056 | Acc Train: 0.7357 | Loss Val: 0.6517 | Acc Val: 0.8400 | Time: 0.1956s\n",
      "Epoch: 0053 | Loss Train: 0.9006 | Acc Train: 0.6643 | Loss Val: 0.6494 | Acc Val: 0.8400 | Time: 0.1955s\n",
      "Epoch: 0054 | Loss Train: 0.8347 | Acc Train: 0.7286 | Loss Val: 0.6472 | Acc Val: 0.8367 | Time: 0.1959s\n",
      "Epoch: 0055 | Loss Train: 0.8067 | Acc Train: 0.7000 | Loss Val: 0.6450 | Acc Val: 0.8367 | Time: 0.1947s\n",
      "Epoch: 0056 | Loss Train: 0.7630 | Acc Train: 0.7286 | Loss Val: 0.6426 | Acc Val: 0.8333 | Time: 0.1968s\n",
      "Epoch: 0057 | Loss Train: 0.7718 | Acc Train: 0.7357 | Loss Val: 0.6396 | Acc Val: 0.8333 | Time: 0.1968s\n",
      "Epoch: 0058 | Loss Train: 0.8162 | Acc Train: 0.7071 | Loss Val: 0.6366 | Acc Val: 0.8333 | Time: 0.1967s\n",
      "Epoch: 0059 | Loss Train: 0.7802 | Acc Train: 0.6929 | Loss Val: 0.6332 | Acc Val: 0.8333 | Time: 0.1946s\n",
      "Epoch: 0060 | Loss Train: 0.9177 | Acc Train: 0.6643 | Loss Val: 0.6303 | Acc Val: 0.8333 | Time: 0.1956s\n",
      "Epoch: 0061 | Loss Train: 0.7300 | Acc Train: 0.7286 | Loss Val: 0.6285 | Acc Val: 0.8333 | Time: 0.1963s\n",
      "Epoch: 0062 | Loss Train: 0.8231 | Acc Train: 0.6929 | Loss Val: 0.6278 | Acc Val: 0.8333 | Time: 0.1990s\n",
      "Epoch: 0063 | Loss Train: 0.7020 | Acc Train: 0.7643 | Loss Val: 0.6272 | Acc Val: 0.8333 | Time: 0.1952s\n",
      "Epoch: 0064 | Loss Train: 0.7971 | Acc Train: 0.7286 | Loss Val: 0.6259 | Acc Val: 0.8333 | Time: 0.1974s\n",
      "Epoch: 0065 | Loss Train: 0.7596 | Acc Train: 0.7286 | Loss Val: 0.6246 | Acc Val: 0.8333 | Time: 0.1944s\n",
      "Epoch: 0066 | Loss Train: 0.9329 | Acc Train: 0.6500 | Loss Val: 0.6226 | Acc Val: 0.8333 | Time: 0.1978s\n",
      "Epoch: 0067 | Loss Train: 0.8566 | Acc Train: 0.7143 | Loss Val: 0.6198 | Acc Val: 0.8333 | Time: 0.1939s\n",
      "Epoch: 0068 | Loss Train: 0.8323 | Acc Train: 0.6714 | Loss Val: 0.6167 | Acc Val: 0.8367 | Time: 0.1971s\n",
      "Epoch: 0069 | Loss Train: 0.7495 | Acc Train: 0.7143 | Loss Val: 0.6144 | Acc Val: 0.8367 | Time: 0.1953s\n",
      "Epoch: 0070 | Loss Train: 0.6895 | Acc Train: 0.7571 | Loss Val: 0.6127 | Acc Val: 0.8367 | Time: 0.1966s\n",
      "Epoch: 0071 | Loss Train: 0.7850 | Acc Train: 0.7214 | Loss Val: 0.6115 | Acc Val: 0.8400 | Time: 0.1940s\n",
      "Epoch: 0072 | Loss Train: 0.8449 | Acc Train: 0.7214 | Loss Val: 0.6109 | Acc Val: 0.8400 | Time: 0.1976s\n",
      "Epoch: 0073 | Loss Train: 0.6915 | Acc Train: 0.7500 | Loss Val: 0.6106 | Acc Val: 0.8333 | Time: 0.1953s\n",
      "Epoch: 0074 | Loss Train: 0.5988 | Acc Train: 0.8000 | Loss Val: 0.6107 | Acc Val: 0.8300 | Time: 0.1988s\n",
      "Epoch: 0075 | Loss Train: 0.7406 | Acc Train: 0.7500 | Loss Val: 0.6111 | Acc Val: 0.8267 | Time: 0.1943s\n",
      "Epoch: 0076 | Loss Train: 0.7333 | Acc Train: 0.7357 | Loss Val: 0.6121 | Acc Val: 0.8267 | Time: 0.1971s\n",
      "Epoch: 0077 | Loss Train: 0.7123 | Acc Train: 0.7500 | Loss Val: 0.6119 | Acc Val: 0.8267 | Time: 0.1957s\n",
      "Epoch: 0078 | Loss Train: 0.8174 | Acc Train: 0.6929 | Loss Val: 0.6094 | Acc Val: 0.8267 | Time: 0.1956s\n",
      "Epoch: 0079 | Loss Train: 0.6507 | Acc Train: 0.7714 | Loss Val: 0.6067 | Acc Val: 0.8300 | Time: 0.1937s\n",
      "Epoch: 0080 | Loss Train: 0.7161 | Acc Train: 0.7500 | Loss Val: 0.6043 | Acc Val: 0.8367 | Time: 0.1982s\n",
      "Epoch: 0081 | Loss Train: 0.6778 | Acc Train: 0.7571 | Loss Val: 0.6017 | Acc Val: 0.8367 | Time: 0.1951s\n",
      "Epoch: 0082 | Loss Train: 0.6949 | Acc Train: 0.7429 | Loss Val: 0.5993 | Acc Val: 0.8400 | Time: 0.1963s\n",
      "Epoch: 0083 | Loss Train: 0.6812 | Acc Train: 0.7286 | Loss Val: 0.5973 | Acc Val: 0.8400 | Time: 0.1968s\n",
      "Epoch: 0084 | Loss Train: 0.7803 | Acc Train: 0.6929 | Loss Val: 0.5951 | Acc Val: 0.8400 | Time: 0.1987s\n",
      "Epoch: 0085 | Loss Train: 0.6993 | Acc Train: 0.7429 | Loss Val: 0.5929 | Acc Val: 0.8400 | Time: 0.1944s\n",
      "Epoch: 0086 | Loss Train: 0.6200 | Acc Train: 0.7571 | Loss Val: 0.5907 | Acc Val: 0.8367 | Time: 0.1964s\n",
      "Epoch: 0087 | Loss Train: 0.7443 | Acc Train: 0.7214 | Loss Val: 0.5892 | Acc Val: 0.8367 | Time: 0.1948s\n",
      "Epoch: 0088 | Loss Train: 0.8184 | Acc Train: 0.6929 | Loss Val: 0.5881 | Acc Val: 0.8367 | Time: 0.1978s\n",
      "Epoch: 0089 | Loss Train: 0.7196 | Acc Train: 0.7429 | Loss Val: 0.5872 | Acc Val: 0.8333 | Time: 0.1942s\n",
      "Epoch: 0090 | Loss Train: 0.6952 | Acc Train: 0.7286 | Loss Val: 0.5863 | Acc Val: 0.8333 | Time: 0.1966s\n",
      "Epoch: 0091 | Loss Train: 0.6578 | Acc Train: 0.7714 | Loss Val: 0.5859 | Acc Val: 0.8333 | Time: 0.1945s\n",
      "Epoch: 0092 | Loss Train: 0.8049 | Acc Train: 0.6857 | Loss Val: 0.5860 | Acc Val: 0.8333 | Time: 0.1962s\n",
      "Epoch: 0093 | Loss Train: 0.7887 | Acc Train: 0.7429 | Loss Val: 0.5867 | Acc Val: 0.8333 | Time: 0.1972s\n",
      "Epoch: 0094 | Loss Train: 0.8692 | Acc Train: 0.6929 | Loss Val: 0.5887 | Acc Val: 0.8333 | Time: 0.1958s\n",
      "Epoch: 0095 | Loss Train: 0.7990 | Acc Train: 0.7071 | Loss Val: 0.5902 | Acc Val: 0.8300 | Time: 0.1943s\n",
      "Epoch: 0096 | Loss Train: 0.6464 | Acc Train: 0.7357 | Loss Val: 0.5910 | Acc Val: 0.8300 | Time: 0.1990s\n",
      "Epoch: 0097 | Loss Train: 0.6758 | Acc Train: 0.7571 | Loss Val: 0.5922 | Acc Val: 0.8300 | Time: 0.1923s\n",
      "Epoch: 0098 | Loss Train: 0.7568 | Acc Train: 0.7143 | Loss Val: 0.5943 | Acc Val: 0.8267 | Time: 0.1996s\n",
      "Epoch: 0099 | Loss Train: 0.8104 | Acc Train: 0.6786 | Loss Val: 0.5970 | Acc Val: 0.8267 | Time: 0.1962s\n",
      "Epoch: 0100 | Loss Train: 0.8684 | Acc Train: 0.6786 | Loss Val: 0.5991 | Acc Val: 0.8267 | Time: 0.1968s\n",
      "Epoch: 0101 | Loss Train: 0.6935 | Acc Train: 0.7286 | Loss Val: 0.6008 | Acc Val: 0.8200 | Time: 0.1950s\n",
      "Epoch: 0102 | Loss Train: 0.7236 | Acc Train: 0.7214 | Loss Val: 0.6017 | Acc Val: 0.8233 | Time: 0.1983s\n",
      "Epoch: 0103 | Loss Train: 0.7280 | Acc Train: 0.7286 | Loss Val: 0.6022 | Acc Val: 0.8267 | Time: 0.1942s\n",
      "Epoch: 0104 | Loss Train: 0.7701 | Acc Train: 0.7071 | Loss Val: 0.6024 | Acc Val: 0.8267 | Time: 0.1974s\n",
      "Epoch: 0105 | Loss Train: 0.7389 | Acc Train: 0.7000 | Loss Val: 0.6023 | Acc Val: 0.8267 | Time: 0.1953s\n",
      "Epoch: 0106 | Loss Train: 0.6995 | Acc Train: 0.7500 | Loss Val: 0.6016 | Acc Val: 0.8233 | Time: 0.1965s\n",
      "Epoch: 0107 | Loss Train: 0.7269 | Acc Train: 0.7500 | Loss Val: 0.6020 | Acc Val: 0.8200 | Time: 0.1956s\n",
      "Epoch: 0108 | Loss Train: 0.7197 | Acc Train: 0.7286 | Loss Val: 0.6022 | Acc Val: 0.8200 | Time: 0.1991s\n",
      "Epoch: 0109 | Loss Train: 0.7302 | Acc Train: 0.7571 | Loss Val: 0.6039 | Acc Val: 0.8267 | Time: 0.1949s\n",
      "Epoch: 0110 | Loss Train: 0.6428 | Acc Train: 0.7857 | Loss Val: 0.6053 | Acc Val: 0.8267 | Time: 0.1946s\n",
      "Epoch: 0111 | Loss Train: 0.7058 | Acc Train: 0.7286 | Loss Val: 0.6053 | Acc Val: 0.8267 | Time: 0.1947s\n",
      "Epoch: 0112 | Loss Train: 0.6856 | Acc Train: 0.7143 | Loss Val: 0.6055 | Acc Val: 0.8267 | Time: 0.1963s\n",
      "Epoch: 0113 | Loss Train: 0.6333 | Acc Train: 0.7714 | Loss Val: 0.6051 | Acc Val: 0.8267 | Time: 0.1954s\n",
      "Epoch: 0114 | Loss Train: 0.7474 | Acc Train: 0.7000 | Loss Val: 0.6027 | Acc Val: 0.8267 | Time: 0.2027s\n",
      "Epoch: 0115 | Loss Train: 0.6751 | Acc Train: 0.7429 | Loss Val: 0.5993 | Acc Val: 0.8267 | Time: 0.1998s\n",
      "Epoch: 0116 | Loss Train: 0.7662 | Acc Train: 0.6857 | Loss Val: 0.5968 | Acc Val: 0.8267 | Time: 0.2020s\n",
      "Epoch: 0117 | Loss Train: 0.6884 | Acc Train: 0.7571 | Loss Val: 0.5936 | Acc Val: 0.8267 | Time: 0.2003s\n",
      "Epoch: 0118 | Loss Train: 0.6334 | Acc Train: 0.7857 | Loss Val: 0.5908 | Acc Val: 0.8267 | Time: 0.1995s\n",
      "Epoch: 0119 | Loss Train: 0.6081 | Acc Train: 0.7500 | Loss Val: 0.5875 | Acc Val: 0.8267 | Time: 0.2002s\n",
      "Epoch: 0120 | Loss Train: 0.8024 | Acc Train: 0.6786 | Loss Val: 0.5844 | Acc Val: 0.8300 | Time: 0.2014s\n",
      "Epoch: 0121 | Loss Train: 0.5759 | Acc Train: 0.8143 | Loss Val: 0.5827 | Acc Val: 0.8300 | Time: 0.1983s\n",
      "Epoch: 0122 | Loss Train: 0.6660 | Acc Train: 0.7357 | Loss Val: 0.5810 | Acc Val: 0.8300 | Time: 0.1951s\n",
      "Epoch: 0123 | Loss Train: 0.6633 | Acc Train: 0.7714 | Loss Val: 0.5803 | Acc Val: 0.8300 | Time: 0.1962s\n",
      "Epoch: 0124 | Loss Train: 0.5713 | Acc Train: 0.7929 | Loss Val: 0.5795 | Acc Val: 0.8300 | Time: 0.1983s\n",
      "Epoch: 0125 | Loss Train: 0.7286 | Acc Train: 0.7214 | Loss Val: 0.5795 | Acc Val: 0.8300 | Time: 0.1986s\n",
      "Epoch: 0126 | Loss Train: 0.7299 | Acc Train: 0.7286 | Loss Val: 0.5793 | Acc Val: 0.8300 | Time: 0.1997s\n",
      "Epoch: 0127 | Loss Train: 0.7121 | Acc Train: 0.7214 | Loss Val: 0.5788 | Acc Val: 0.8300 | Time: 0.1928s\n",
      "Epoch: 0128 | Loss Train: 0.6135 | Acc Train: 0.7429 | Loss Val: 0.5793 | Acc Val: 0.8300 | Time: 0.1952s\n",
      "Epoch: 0129 | Loss Train: 0.5766 | Acc Train: 0.7571 | Loss Val: 0.5800 | Acc Val: 0.8300 | Time: 0.2007s\n",
      "Epoch: 0130 | Loss Train: 0.6955 | Acc Train: 0.6929 | Loss Val: 0.5808 | Acc Val: 0.8233 | Time: 0.1990s\n",
      "Epoch: 0131 | Loss Train: 0.6627 | Acc Train: 0.7429 | Loss Val: 0.5827 | Acc Val: 0.8200 | Time: 0.2003s\n",
      "Epoch: 0132 | Loss Train: 0.7411 | Acc Train: 0.7000 | Loss Val: 0.5837 | Acc Val: 0.8200 | Time: 0.1970s\n",
      "Epoch: 0133 | Loss Train: 0.6915 | Acc Train: 0.7214 | Loss Val: 0.5848 | Acc Val: 0.8200 | Time: 0.2034s\n",
      "Epoch: 0134 | Loss Train: 0.6849 | Acc Train: 0.7357 | Loss Val: 0.5856 | Acc Val: 0.8133 | Time: 0.1938s\n",
      "Epoch: 0135 | Loss Train: 0.6654 | Acc Train: 0.7500 | Loss Val: 0.5858 | Acc Val: 0.8133 | Time: 0.1941s\n",
      "Epoch: 0136 | Loss Train: 0.7464 | Acc Train: 0.7143 | Loss Val: 0.5841 | Acc Val: 0.8133 | Time: 0.2010s\n",
      "Epoch: 0137 | Loss Train: 0.6879 | Acc Train: 0.7357 | Loss Val: 0.5803 | Acc Val: 0.8167 | Time: 0.1941s\n",
      "Epoch: 0138 | Loss Train: 0.6902 | Acc Train: 0.7143 | Loss Val: 0.5770 | Acc Val: 0.8200 | Time: 0.1967s\n",
      "Epoch: 0139 | Loss Train: 0.6693 | Acc Train: 0.7357 | Loss Val: 0.5751 | Acc Val: 0.8233 | Time: 0.1957s\n",
      "Epoch: 0140 | Loss Train: 0.5836 | Acc Train: 0.7786 | Loss Val: 0.5738 | Acc Val: 0.8233 | Time: 0.1941s\n",
      "Epoch: 0141 | Loss Train: 0.7186 | Acc Train: 0.7571 | Loss Val: 0.5732 | Acc Val: 0.8200 | Time: 0.1950s\n",
      "Epoch: 0142 | Loss Train: 0.6573 | Acc Train: 0.7714 | Loss Val: 0.5741 | Acc Val: 0.8200 | Time: 0.1946s\n",
      "Epoch: 0143 | Loss Train: 0.6809 | Acc Train: 0.7500 | Loss Val: 0.5753 | Acc Val: 0.8200 | Time: 0.1955s\n",
      "Epoch: 0144 | Loss Train: 0.5581 | Acc Train: 0.7786 | Loss Val: 0.5767 | Acc Val: 0.8167 | Time: 0.1946s\n",
      "Epoch: 0145 | Loss Train: 0.6355 | Acc Train: 0.7357 | Loss Val: 0.5779 | Acc Val: 0.8133 | Time: 0.1999s\n",
      "Epoch: 0146 | Loss Train: 0.6411 | Acc Train: 0.7429 | Loss Val: 0.5791 | Acc Val: 0.8133 | Time: 0.1994s\n",
      "Epoch: 0147 | Loss Train: 0.5851 | Acc Train: 0.7857 | Loss Val: 0.5804 | Acc Val: 0.8133 | Time: 0.1978s\n",
      "Epoch: 0148 | Loss Train: 0.6791 | Acc Train: 0.7214 | Loss Val: 0.5816 | Acc Val: 0.8133 | Time: 0.1961s\n",
      "Epoch: 0149 | Loss Train: 0.6487 | Acc Train: 0.7571 | Loss Val: 0.5823 | Acc Val: 0.8100 | Time: 0.1949s\n",
      "Epoch: 0150 | Loss Train: 0.8049 | Acc Train: 0.7143 | Loss Val: 0.5826 | Acc Val: 0.8167 | Time: 0.1952s\n",
      "Epoch: 0151 | Loss Train: 0.6926 | Acc Train: 0.7071 | Loss Val: 0.5830 | Acc Val: 0.8167 | Time: 0.1974s\n",
      "Epoch: 0152 | Loss Train: 0.7104 | Acc Train: 0.7286 | Loss Val: 0.5829 | Acc Val: 0.8167 | Time: 0.1952s\n",
      "Epoch: 0153 | Loss Train: 0.7604 | Acc Train: 0.7071 | Loss Val: 0.5829 | Acc Val: 0.8167 | Time: 0.1948s\n",
      "Epoch: 0154 | Loss Train: 0.6329 | Acc Train: 0.7929 | Loss Val: 0.5834 | Acc Val: 0.8133 | Time: 0.1938s\n",
      "Epoch: 0155 | Loss Train: 0.6573 | Acc Train: 0.7429 | Loss Val: 0.5834 | Acc Val: 0.8133 | Time: 0.1958s\n",
      "Epoch: 0156 | Loss Train: 0.8174 | Acc Train: 0.6786 | Loss Val: 0.5830 | Acc Val: 0.8133 | Time: 0.1993s\n",
      "Epoch: 0157 | Loss Train: 0.7202 | Acc Train: 0.7500 | Loss Val: 0.5834 | Acc Val: 0.8133 | Time: 0.1966s\n",
      "Epoch: 0158 | Loss Train: 0.7151 | Acc Train: 0.7571 | Loss Val: 0.5848 | Acc Val: 0.8133 | Time: 0.1940s\n",
      "Epoch: 0159 | Loss Train: 0.6842 | Acc Train: 0.7214 | Loss Val: 0.5864 | Acc Val: 0.8100 | Time: 0.1991s\n",
      "Epoch: 0160 | Loss Train: 0.7170 | Acc Train: 0.7429 | Loss Val: 0.5885 | Acc Val: 0.8100 | Time: 0.1992s\n",
      "Epoch: 0161 | Loss Train: 0.6848 | Acc Train: 0.7286 | Loss Val: 0.5897 | Acc Val: 0.8100 | Time: 0.1950s\n",
      "Epoch: 0162 | Loss Train: 0.6084 | Acc Train: 0.7643 | Loss Val: 0.5912 | Acc Val: 0.8067 | Time: 0.1943s\n",
      "Epoch: 0163 | Loss Train: 0.5792 | Acc Train: 0.7929 | Loss Val: 0.5930 | Acc Val: 0.8067 | Time: 0.1968s\n",
      "Epoch: 0164 | Loss Train: 0.7278 | Acc Train: 0.7214 | Loss Val: 0.5958 | Acc Val: 0.8067 | Time: 0.1951s\n",
      "Epoch: 0165 | Loss Train: 0.6152 | Acc Train: 0.7500 | Loss Val: 0.5979 | Acc Val: 0.8067 | Time: 0.1954s\n",
      "Epoch: 0166 | Loss Train: 0.6308 | Acc Train: 0.7357 | Loss Val: 0.5994 | Acc Val: 0.8067 | Time: 0.1947s\n",
      "Epoch: 0167 | Loss Train: 0.7008 | Acc Train: 0.7286 | Loss Val: 0.5995 | Acc Val: 0.8067 | Time: 0.1993s\n",
      "Epoch: 0168 | Loss Train: 0.5679 | Acc Train: 0.7929 | Loss Val: 0.5980 | Acc Val: 0.8067 | Time: 0.1958s\n",
      "Epoch: 0169 | Loss Train: 0.7086 | Acc Train: 0.7571 | Loss Val: 0.5985 | Acc Val: 0.8033 | Time: 0.1963s\n",
      "Epoch: 0170 | Loss Train: 0.6531 | Acc Train: 0.7357 | Loss Val: 0.5969 | Acc Val: 0.8033 | Time: 0.1989s\n",
      "Epoch: 0171 | Loss Train: 0.5814 | Acc Train: 0.8000 | Loss Val: 0.5954 | Acc Val: 0.8033 | Time: 0.2016s\n",
      "Epoch: 0172 | Loss Train: 0.6901 | Acc Train: 0.7071 | Loss Val: 0.5945 | Acc Val: 0.8033 | Time: 0.1973s\n",
      "Epoch: 0173 | Loss Train: 0.6470 | Acc Train: 0.7500 | Loss Val: 0.5934 | Acc Val: 0.8033 | Time: 0.1940s\n",
      "Epoch: 0174 | Loss Train: 0.5035 | Acc Train: 0.8000 | Loss Val: 0.5916 | Acc Val: 0.8033 | Time: 0.1952s\n",
      "Epoch: 0175 | Loss Train: 0.7253 | Acc Train: 0.7500 | Loss Val: 0.5902 | Acc Val: 0.8067 | Time: 0.1957s\n",
      "Epoch: 0176 | Loss Train: 0.6095 | Acc Train: 0.7500 | Loss Val: 0.5877 | Acc Val: 0.8133 | Time: 0.1948s\n",
      "Epoch: 0177 | Loss Train: 0.6347 | Acc Train: 0.7357 | Loss Val: 0.5862 | Acc Val: 0.8133 | Time: 0.1948s\n",
      "Epoch: 0178 | Loss Train: 0.6196 | Acc Train: 0.7357 | Loss Val: 0.5843 | Acc Val: 0.8167 | Time: 0.1952s\n",
      "Epoch: 0179 | Loss Train: 0.4727 | Acc Train: 0.8071 | Loss Val: 0.5822 | Acc Val: 0.8167 | Time: 0.1949s\n",
      "Epoch: 0180 | Loss Train: 0.6530 | Acc Train: 0.7857 | Loss Val: 0.5820 | Acc Val: 0.8167 | Time: 0.1949s\n",
      "Epoch: 0181 | Loss Train: 0.6579 | Acc Train: 0.7643 | Loss Val: 0.5817 | Acc Val: 0.8167 | Time: 0.2008s\n",
      "Epoch: 0182 | Loss Train: 0.7189 | Acc Train: 0.7071 | Loss Val: 0.5812 | Acc Val: 0.8167 | Time: 0.1996s\n",
      "Epoch: 0183 | Loss Train: 0.7931 | Acc Train: 0.7071 | Loss Val: 0.5804 | Acc Val: 0.8167 | Time: 0.1955s\n",
      "Epoch: 0184 | Loss Train: 0.6059 | Acc Train: 0.7500 | Loss Val: 0.5807 | Acc Val: 0.8133 | Time: 0.1979s\n",
      "Epoch: 0185 | Loss Train: 0.6994 | Acc Train: 0.7071 | Loss Val: 0.5812 | Acc Val: 0.8100 | Time: 0.1962s\n",
      "Epoch: 0186 | Loss Train: 0.6287 | Acc Train: 0.7571 | Loss Val: 0.5821 | Acc Val: 0.8100 | Time: 0.1958s\n",
      "Epoch: 0187 | Loss Train: 0.6016 | Acc Train: 0.7786 | Loss Val: 0.5832 | Acc Val: 0.8067 | Time: 0.1948s\n",
      "Epoch: 0188 | Loss Train: 0.6490 | Acc Train: 0.7357 | Loss Val: 0.5835 | Acc Val: 0.8067 | Time: 0.1944s\n",
      "Epoch: 0189 | Loss Train: 0.6790 | Acc Train: 0.7429 | Loss Val: 0.5835 | Acc Val: 0.8067 | Time: 0.2160s\n",
      "Epoch: 0190 | Loss Train: 0.6519 | Acc Train: 0.7357 | Loss Val: 0.5834 | Acc Val: 0.8100 | Time: 0.1949s\n",
      "Epoch: 0191 | Loss Train: 0.6818 | Acc Train: 0.7357 | Loss Val: 0.5824 | Acc Val: 0.8133 | Time: 0.1952s\n",
      "Epoch: 0192 | Loss Train: 0.5813 | Acc Train: 0.7571 | Loss Val: 0.5819 | Acc Val: 0.8167 | Time: 0.1951s\n",
      "Epoch: 0193 | Loss Train: 0.7049 | Acc Train: 0.7286 | Loss Val: 0.5819 | Acc Val: 0.8200 | Time: 0.2028s\n",
      "Epoch: 0194 | Loss Train: 0.5906 | Acc Train: 0.7429 | Loss Val: 0.5802 | Acc Val: 0.8200 | Time: 0.2017s\n",
      "Epoch: 0195 | Loss Train: 0.5939 | Acc Train: 0.7571 | Loss Val: 0.5788 | Acc Val: 0.8233 | Time: 0.1971s\n",
      "Epoch: 0196 | Loss Train: 0.6686 | Acc Train: 0.7286 | Loss Val: 0.5768 | Acc Val: 0.8267 | Time: 0.2043s\n",
      "Epoch: 0197 | Loss Train: 0.5809 | Acc Train: 0.7714 | Loss Val: 0.5748 | Acc Val: 0.8267 | Time: 0.2011s\n",
      "Epoch: 0198 | Loss Train: 0.6113 | Acc Train: 0.7714 | Loss Val: 0.5736 | Acc Val: 0.8267 | Time: 0.1965s\n",
      "Epoch: 0199 | Loss Train: 0.6027 | Acc Train: 0.7714 | Loss Val: 0.5727 | Acc Val: 0.8267 | Time: 0.1959s\n",
      "Epoch: 0200 | Loss Train: 0.6875 | Acc Train: 0.7429 | Loss Val: 0.5730 | Acc Val: 0.8267 | Time: 0.1948s\n",
      "Training finished. Total time: 40.4462s\n",
      "Loading best model weights...\n",
      "\n",
      "Test Set Results: loss= 0.5904, accuracy= 0.8210\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# --- HYPERPARAMETERS ---\n",
    "# Parameters taken directly from the GAT paper for Cora\n",
    "LR = 0.005              # Learning rate\n",
    "WEIGHT_DECAY = 5e-4     # L2 Regularization (Crucial for small datasets)\n",
    "EPOCHS = 200            # Max epochs\n",
    "PATIENCE = 100          # Early stopping patience\n",
    "\n",
    "# Define Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Loss Function (Negative Log Likelihood)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    \"\"\"Computes accuracy of the model predictions.\"\"\"\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "def train(epoch):\n",
    "    \"\"\"\n",
    "    Training logic for one epoch.\n",
    "    \"\"\"\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(features, adj)\n",
    "    \n",
    "    # Calculate loss and accuracy ONLY on training nodes\n",
    "    loss_train = criterion(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    \n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation phase (No gradient calculation needed)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(features, adj)\n",
    "        loss_val = criterion(output[idx_val], labels[idx_val])\n",
    "        acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "        \n",
    "    print(f'Epoch: {epoch+1:04d} | '\n",
    "          f'Loss Train: {loss_train.item():.4f} | '\n",
    "          f'Acc Train: {acc_train.item():.4f} | '\n",
    "          f'Loss Val: {loss_val.item():.4f} | '\n",
    "          f'Acc Val: {acc_val.item():.4f} | '\n",
    "          f'Time: {time.time() - t:.4f}s')\n",
    "          \n",
    "    return loss_train.item(), loss_val.item(), acc_val.item()\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    Final testing logic on the test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(features, adj)\n",
    "        loss_test = criterion(output[idx_test], labels[idx_test])\n",
    "        acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "        \n",
    "    print(f\"\\nTest Set Results: \"\n",
    "          f\"loss= {loss_test.item():.4f}, \"\n",
    "          f\"accuracy= {acc_test.item():.4f}\")\n",
    "    \n",
    "    return acc_test.item()\n",
    "\n",
    "# --- MAIN TRAINING LOOP ---\n",
    "print(\"Starting training...\")\n",
    "loss_history_train = []\n",
    "loss_history_val = []\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "start_total = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, val_loss, val_acc = train(epoch)\n",
    "    loss_history_train.append(train_loss)\n",
    "    loss_history_val.append(val_loss)\n",
    "\n",
    "    # Save the best model (Early Stopping Logic)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_gat_cora.pkl')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(f\"Training finished. Total time: {time.time() - start_total:.4f}s\")\n",
    "\n",
    "# Load the best model weights\n",
    "print(\"Loading best model weights...\")\n",
    "model.load_state_dict(torch.load('best_gat_cora.pkl'))\n",
    "\n",
    "# Run final test\n",
    "final_acc = test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1296b076-ad69-470f-82d3-a261fb1f2425",
   "metadata": {},
   "source": [
    "# Our Contribution part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cc5b318-2621-451b-9004-81f52076c50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom GAT model (with Head Attention) defined successfully.\n"
     ]
    }
   ],
   "source": [
    "class GAT_With_HeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced GAT model with 'Attention on Heads' mechanism.\n",
    "    \n",
    "    Difference from Baseline:\n",
    "    - The output layer is NOT a single head. It consists of multiple heads.\n",
    "    - Instead of averaging the outputs of these heads (standard GAT), \n",
    "      we learn a dynamic weight for each head using a secondary attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        super(GAT_With_HeadAttention, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # --- Layer 1: Standard Multi-Head Attention (Same as Baseline) ---\n",
    "        self.attentions = nn.ModuleList([\n",
    "            GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) \n",
    "            for _ in range(nheads)\n",
    "        ])\n",
    "\n",
    "        # --- Layer 2: Multi-Head Output (Modified) ---\n",
    "        # In baseline, this was a single layer. Here we use multiple heads \n",
    "        # to generate candidate predictions, which we will then weight.\n",
    "        self.out_heads = nn.ModuleList([\n",
    "            GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n",
    "            for _ in range(nheads)\n",
    "        ])\n",
    "        \n",
    "        # --- Our Contribution: Head Attention Mechanism ---\n",
    "        # This small linear layer will learn \"How important is this head for this node?\"\n",
    "        # Input: The output features of a head (nclass dimension)\n",
    "        # Output: A scalar importance score\n",
    "        self.head_att_weights = nn.Linear(nclass, 1)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        \n",
    "        # Layer 1: Standard Concatenation\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        \n",
    "        # Layer 2: Get outputs from ALL output heads separately\n",
    "        # Each head_out is (N, nclass)\n",
    "        # We stack them to get (N, nheads, nclass)\n",
    "        head_outputs = torch.stack([head(x, adj) for head in self.out_heads], dim=1)\n",
    "        \n",
    "        # --- APPLYING ATTENTION ON HEADS ---\n",
    "        \n",
    "        # 1. Calculate Importance Scores\n",
    "        # Pass each head's output through the linear layer\n",
    "        # Input: (N, nheads, nclass) -> Output: (N, nheads, 1)\n",
    "        attn_scores = self.head_att_weights(head_outputs)\n",
    "        \n",
    "        # 2. Normalize Scores (Softmax over the 'heads' dimension)\n",
    "        # We want weights across the heads to sum to 1 for each node.\n",
    "        attn_weights = F.softmax(attn_scores, dim=1) \n",
    "        \n",
    "        # 3. Weighted Aggregation\n",
    "        # Weighted Sum: sum(weight_k * output_k)\n",
    "        # (N, nheads, 1) * (N, nheads, nclass) -> (N, nheads, nclass) -> sum dim 1 -> (N, nclass)\n",
    "        final_output = torch.sum(attn_weights * head_outputs, dim=1)\n",
    "        \n",
    "        return F.log_softmax(final_output, dim=1)\n",
    "\n",
    "print(\"Custom GAT model (with Head Attention) defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e66d40e1-ff43-4ef1-b6c8-0a81f447d382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Model Architecture:\n",
      "GAT_With_HeadAttention(\n",
      "  (attentions): ModuleList(\n",
      "    (0-7): 8 x GraphAttentionLayer(\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "  )\n",
      "  (out_heads): ModuleList(\n",
      "    (0-7): 8 x GraphAttentionLayer(\n",
      "      (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "  )\n",
      "  (head_att_weights): Linear(in_features=7, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "Starting training for Custom Model...\n",
      "Epoch: 0001 | Loss Train: 2.0182 | Acc Train: 0.1286 | Loss Val: 1.9196 | Acc Val: 0.2800 | Time: 7.0599s\n",
      "Epoch: 0002 | Loss Train: 1.9897 | Acc Train: 0.1857 | Loss Val: 1.8858 | Acc Val: 0.4400 | Time: 7.4595s\n",
      "Epoch: 0003 | Loss Train: 1.9669 | Acc Train: 0.1929 | Loss Val: 1.8528 | Acc Val: 0.5333 | Time: 7.4970s\n",
      "Epoch: 0004 | Loss Train: 1.9028 | Acc Train: 0.2714 | Loss Val: 1.8199 | Acc Val: 0.5433 | Time: 7.4734s\n",
      "Epoch: 0005 | Loss Train: 1.9195 | Acc Train: 0.2786 | Loss Val: 1.7891 | Acc Val: 0.5567 | Time: 7.4989s\n",
      "Epoch: 0006 | Loss Train: 1.8252 | Acc Train: 0.3357 | Loss Val: 1.7596 | Acc Val: 0.5833 | Time: 7.4730s\n",
      "Epoch: 0007 | Loss Train: 1.7557 | Acc Train: 0.4357 | Loss Val: 1.7301 | Acc Val: 0.5833 | Time: 7.5260s\n",
      "Epoch: 0008 | Loss Train: 1.7523 | Acc Train: 0.4357 | Loss Val: 1.7010 | Acc Val: 0.6000 | Time: 7.4781s\n",
      "Epoch: 0009 | Loss Train: 1.7494 | Acc Train: 0.4643 | Loss Val: 1.6721 | Acc Val: 0.6000 | Time: 7.5178s\n",
      "Epoch: 0010 | Loss Train: 1.5974 | Acc Train: 0.5357 | Loss Val: 1.6434 | Acc Val: 0.6067 | Time: 7.4760s\n",
      "Epoch: 0011 | Loss Train: 1.6165 | Acc Train: 0.4857 | Loss Val: 1.6147 | Acc Val: 0.6033 | Time: 7.4990s\n",
      "Epoch: 0012 | Loss Train: 1.6194 | Acc Train: 0.4786 | Loss Val: 1.5859 | Acc Val: 0.6067 | Time: 7.4678s\n",
      "Epoch: 0013 | Loss Train: 1.5445 | Acc Train: 0.4714 | Loss Val: 1.5570 | Acc Val: 0.6100 | Time: 7.4908s\n",
      "Epoch: 0014 | Loss Train: 1.4901 | Acc Train: 0.5143 | Loss Val: 1.5287 | Acc Val: 0.6167 | Time: 7.5302s\n",
      "Epoch: 0015 | Loss Train: 1.5780 | Acc Train: 0.4786 | Loss Val: 1.5015 | Acc Val: 0.6200 | Time: 7.5082s\n",
      "Epoch: 0016 | Loss Train: 1.4216 | Acc Train: 0.5214 | Loss Val: 1.4742 | Acc Val: 0.6233 | Time: 7.4858s\n",
      "Epoch: 0017 | Loss Train: 1.4579 | Acc Train: 0.5714 | Loss Val: 1.4476 | Acc Val: 0.6267 | Time: 7.4657s\n",
      "Epoch: 0018 | Loss Train: 1.3990 | Acc Train: 0.5786 | Loss Val: 1.4208 | Acc Val: 0.6267 | Time: 7.4870s\n",
      "Epoch: 0019 | Loss Train: 1.3496 | Acc Train: 0.6000 | Loss Val: 1.3949 | Acc Val: 0.6300 | Time: 7.4771s\n",
      "Epoch: 0020 | Loss Train: 1.2537 | Acc Train: 0.6357 | Loss Val: 1.3697 | Acc Val: 0.6333 | Time: 7.5289s\n",
      "Epoch: 0021 | Loss Train: 1.4697 | Acc Train: 0.5643 | Loss Val: 1.3456 | Acc Val: 0.6533 | Time: 7.4564s\n",
      "Epoch: 0022 | Loss Train: 1.2251 | Acc Train: 0.6429 | Loss Val: 1.3220 | Acc Val: 0.6600 | Time: 7.4960s\n",
      "Epoch: 0023 | Loss Train: 1.3179 | Acc Train: 0.5857 | Loss Val: 1.2996 | Acc Val: 0.6733 | Time: 7.4956s\n",
      "Epoch: 0024 | Loss Train: 1.2587 | Acc Train: 0.6786 | Loss Val: 1.2786 | Acc Val: 0.6800 | Time: 7.5056s\n",
      "Epoch: 0025 | Loss Train: 1.2079 | Acc Train: 0.6786 | Loss Val: 1.2582 | Acc Val: 0.6867 | Time: 7.5074s\n",
      "Epoch: 0026 | Loss Train: 1.0833 | Acc Train: 0.7000 | Loss Val: 1.2377 | Acc Val: 0.6967 | Time: 7.4974s\n",
      "Epoch: 0027 | Loss Train: 1.1800 | Acc Train: 0.7357 | Loss Val: 1.2175 | Acc Val: 0.6967 | Time: 7.4843s\n",
      "Epoch: 0028 | Loss Train: 1.1278 | Acc Train: 0.7643 | Loss Val: 1.1982 | Acc Val: 0.7167 | Time: 7.5321s\n",
      "Epoch: 0029 | Loss Train: 1.0790 | Acc Train: 0.7000 | Loss Val: 1.1791 | Acc Val: 0.7400 | Time: 7.5236s\n",
      "Epoch: 0030 | Loss Train: 0.9984 | Acc Train: 0.7643 | Loss Val: 1.1598 | Acc Val: 0.7533 | Time: 7.5214s\n",
      "Epoch: 0031 | Loss Train: 1.0187 | Acc Train: 0.7214 | Loss Val: 1.1407 | Acc Val: 0.7600 | Time: 7.4842s\n",
      "Epoch: 0032 | Loss Train: 0.9931 | Acc Train: 0.7786 | Loss Val: 1.1220 | Acc Val: 0.7667 | Time: 7.4654s\n",
      "Epoch: 0033 | Loss Train: 0.9405 | Acc Train: 0.7929 | Loss Val: 1.1032 | Acc Val: 0.7733 | Time: 7.5231s\n",
      "Epoch: 0034 | Loss Train: 0.9363 | Acc Train: 0.7714 | Loss Val: 1.0846 | Acc Val: 0.7700 | Time: 7.5878s\n",
      "Epoch: 0035 | Loss Train: 0.8934 | Acc Train: 0.7929 | Loss Val: 1.0656 | Acc Val: 0.7767 | Time: 7.6027s\n",
      "Epoch: 0036 | Loss Train: 0.9637 | Acc Train: 0.7357 | Loss Val: 1.0470 | Acc Val: 0.7800 | Time: 7.6550s\n",
      "Epoch: 0037 | Loss Train: 0.8273 | Acc Train: 0.7714 | Loss Val: 1.0290 | Acc Val: 0.7833 | Time: 7.7865s\n",
      "Epoch: 0038 | Loss Train: 1.0239 | Acc Train: 0.7357 | Loss Val: 1.0119 | Acc Val: 0.7900 | Time: 7.7803s\n",
      "Epoch: 0039 | Loss Train: 0.8082 | Acc Train: 0.8286 | Loss Val: 0.9948 | Acc Val: 0.7933 | Time: 7.8039s\n",
      "Epoch: 0040 | Loss Train: 0.9076 | Acc Train: 0.7643 | Loss Val: 0.9787 | Acc Val: 0.7967 | Time: 7.7822s\n",
      "Epoch: 0041 | Loss Train: 0.7930 | Acc Train: 0.8214 | Loss Val: 0.9631 | Acc Val: 0.7967 | Time: 7.7815s\n",
      "Epoch: 0042 | Loss Train: 0.7868 | Acc Train: 0.8000 | Loss Val: 0.9482 | Acc Val: 0.8067 | Time: 7.7192s\n",
      "Epoch: 0043 | Loss Train: 0.7251 | Acc Train: 0.8643 | Loss Val: 0.9334 | Acc Val: 0.8133 | Time: 7.7725s\n",
      "Epoch: 0044 | Loss Train: 0.7859 | Acc Train: 0.8429 | Loss Val: 0.9198 | Acc Val: 0.8167 | Time: 7.6178s\n",
      "Epoch: 0045 | Loss Train: 0.8498 | Acc Train: 0.8500 | Loss Val: 0.9074 | Acc Val: 0.8167 | Time: 7.5940s\n",
      "Epoch: 0046 | Loss Train: 0.8202 | Acc Train: 0.8571 | Loss Val: 0.8953 | Acc Val: 0.8167 | Time: 7.5699s\n",
      "Epoch: 0047 | Loss Train: 0.6392 | Acc Train: 0.8929 | Loss Val: 0.8829 | Acc Val: 0.8167 | Time: 7.6262s\n",
      "Epoch: 0048 | Loss Train: 0.7370 | Acc Train: 0.8357 | Loss Val: 0.8712 | Acc Val: 0.8233 | Time: 7.4725s\n",
      "Epoch: 0049 | Loss Train: 0.6911 | Acc Train: 0.8286 | Loss Val: 0.8599 | Acc Val: 0.8200 | Time: 7.5018s\n",
      "Epoch: 0050 | Loss Train: 0.6650 | Acc Train: 0.8714 | Loss Val: 0.8493 | Acc Val: 0.8233 | Time: 7.4846s\n",
      "Epoch: 0051 | Loss Train: 0.6004 | Acc Train: 0.8714 | Loss Val: 0.8393 | Acc Val: 0.8267 | Time: 7.4518s\n",
      "Epoch: 0052 | Loss Train: 0.5416 | Acc Train: 0.8929 | Loss Val: 0.8290 | Acc Val: 0.8267 | Time: 7.5024s\n",
      "Epoch: 0053 | Loss Train: 0.6348 | Acc Train: 0.8714 | Loss Val: 0.8194 | Acc Val: 0.8267 | Time: 7.4685s\n",
      "Epoch: 0054 | Loss Train: 0.6279 | Acc Train: 0.8571 | Loss Val: 0.8102 | Acc Val: 0.8267 | Time: 7.4969s\n",
      "Epoch: 0055 | Loss Train: 0.6639 | Acc Train: 0.8714 | Loss Val: 0.8011 | Acc Val: 0.8267 | Time: 7.4755s\n",
      "Epoch: 0056 | Loss Train: 0.5755 | Acc Train: 0.9000 | Loss Val: 0.7923 | Acc Val: 0.8267 | Time: 7.4793s\n",
      "Epoch: 0057 | Loss Train: 0.6661 | Acc Train: 0.8500 | Loss Val: 0.7834 | Acc Val: 0.8300 | Time: 7.4794s\n",
      "Epoch: 0058 | Loss Train: 0.6254 | Acc Train: 0.9071 | Loss Val: 0.7758 | Acc Val: 0.8300 | Time: 7.5565s\n",
      "Epoch: 0059 | Loss Train: 0.5934 | Acc Train: 0.8714 | Loss Val: 0.7687 | Acc Val: 0.8267 | Time: 7.4589s\n",
      "Epoch: 0060 | Loss Train: 0.6063 | Acc Train: 0.8571 | Loss Val: 0.7614 | Acc Val: 0.8233 | Time: 7.5208s\n",
      "Epoch: 0061 | Loss Train: 0.6055 | Acc Train: 0.8357 | Loss Val: 0.7546 | Acc Val: 0.8233 | Time: 7.4739s\n",
      "Epoch: 0062 | Loss Train: 0.5650 | Acc Train: 0.9000 | Loss Val: 0.7485 | Acc Val: 0.8233 | Time: 7.5450s\n",
      "Epoch: 0063 | Loss Train: 0.5797 | Acc Train: 0.8714 | Loss Val: 0.7429 | Acc Val: 0.8267 | Time: 7.5068s\n",
      "Epoch: 0064 | Loss Train: 0.5526 | Acc Train: 0.8643 | Loss Val: 0.7376 | Acc Val: 0.8267 | Time: 7.5021s\n",
      "Epoch: 0065 | Loss Train: 0.4840 | Acc Train: 0.9000 | Loss Val: 0.7323 | Acc Val: 0.8267 | Time: 7.4761s\n",
      "Epoch: 0066 | Loss Train: 0.5394 | Acc Train: 0.8286 | Loss Val: 0.7275 | Acc Val: 0.8300 | Time: 7.5143s\n",
      "Epoch: 0067 | Loss Train: 0.4759 | Acc Train: 0.9143 | Loss Val: 0.7229 | Acc Val: 0.8300 | Time: 7.4868s\n",
      "Epoch: 0068 | Loss Train: 0.5905 | Acc Train: 0.9000 | Loss Val: 0.7187 | Acc Val: 0.8300 | Time: 7.4650s\n",
      "Epoch: 0069 | Loss Train: 0.4947 | Acc Train: 0.9000 | Loss Val: 0.7148 | Acc Val: 0.8300 | Time: 7.4765s\n",
      "Epoch: 0070 | Loss Train: 0.4805 | Acc Train: 0.8786 | Loss Val: 0.7107 | Acc Val: 0.8300 | Time: 7.4923s\n",
      "Epoch: 0071 | Loss Train: 0.5150 | Acc Train: 0.9000 | Loss Val: 0.7073 | Acc Val: 0.8333 | Time: 7.4814s\n",
      "Epoch: 0072 | Loss Train: 0.5413 | Acc Train: 0.8786 | Loss Val: 0.7039 | Acc Val: 0.8300 | Time: 7.4512s\n",
      "Epoch: 0073 | Loss Train: 0.6217 | Acc Train: 0.8429 | Loss Val: 0.6997 | Acc Val: 0.8300 | Time: 7.5184s\n",
      "Epoch: 0074 | Loss Train: 0.5822 | Acc Train: 0.8857 | Loss Val: 0.6959 | Acc Val: 0.8333 | Time: 7.5033s\n",
      "Epoch: 0075 | Loss Train: 0.4851 | Acc Train: 0.8929 | Loss Val: 0.6924 | Acc Val: 0.8333 | Time: 7.5368s\n",
      "Epoch: 0076 | Loss Train: 0.4302 | Acc Train: 0.9357 | Loss Val: 0.6893 | Acc Val: 0.8333 | Time: 7.4799s\n",
      "Epoch: 0077 | Loss Train: 0.4328 | Acc Train: 0.9214 | Loss Val: 0.6861 | Acc Val: 0.8333 | Time: 7.5402s\n",
      "Epoch: 0078 | Loss Train: 0.5657 | Acc Train: 0.9000 | Loss Val: 0.6840 | Acc Val: 0.8333 | Time: 7.5873s\n",
      "Epoch: 0079 | Loss Train: 0.4795 | Acc Train: 0.9071 | Loss Val: 0.6822 | Acc Val: 0.8267 | Time: 7.6299s\n",
      "Epoch: 0080 | Loss Train: 0.4817 | Acc Train: 0.9143 | Loss Val: 0.6806 | Acc Val: 0.8233 | Time: 7.5992s\n",
      "Epoch: 0081 | Loss Train: 0.4450 | Acc Train: 0.8929 | Loss Val: 0.6790 | Acc Val: 0.8233 | Time: 7.7725s\n",
      "Epoch: 0082 | Loss Train: 0.3663 | Acc Train: 0.9429 | Loss Val: 0.6770 | Acc Val: 0.8233 | Time: 7.7597s\n",
      "Epoch: 0083 | Loss Train: 0.4343 | Acc Train: 0.9071 | Loss Val: 0.6751 | Acc Val: 0.8267 | Time: 7.8035s\n",
      "Epoch: 0084 | Loss Train: 0.3521 | Acc Train: 0.9214 | Loss Val: 0.6734 | Acc Val: 0.8267 | Time: 7.7668s\n",
      "Epoch: 0085 | Loss Train: 0.3818 | Acc Train: 0.9071 | Loss Val: 0.6720 | Acc Val: 0.8267 | Time: 7.7834s\n",
      "Epoch: 0086 | Loss Train: 0.4006 | Acc Train: 0.9000 | Loss Val: 0.6703 | Acc Val: 0.8233 | Time: 7.6829s\n",
      "Epoch: 0087 | Loss Train: 0.4083 | Acc Train: 0.9429 | Loss Val: 0.6682 | Acc Val: 0.8233 | Time: 7.6789s\n",
      "Epoch: 0088 | Loss Train: 0.3782 | Acc Train: 0.8929 | Loss Val: 0.6657 | Acc Val: 0.8200 | Time: 7.6639s\n",
      "Epoch: 0089 | Loss Train: 0.4326 | Acc Train: 0.9143 | Loss Val: 0.6630 | Acc Val: 0.8233 | Time: 7.6429s\n",
      "Epoch: 0090 | Loss Train: 0.3572 | Acc Train: 0.9429 | Loss Val: 0.6603 | Acc Val: 0.8233 | Time: 7.6057s\n",
      "Epoch: 0091 | Loss Train: 0.3639 | Acc Train: 0.9286 | Loss Val: 0.6576 | Acc Val: 0.8233 | Time: 7.6508s\n",
      "Epoch: 0092 | Loss Train: 0.4404 | Acc Train: 0.9143 | Loss Val: 0.6555 | Acc Val: 0.8233 | Time: 7.5420s\n",
      "Epoch: 0093 | Loss Train: 0.4003 | Acc Train: 0.9071 | Loss Val: 0.6541 | Acc Val: 0.8267 | Time: 7.5497s\n",
      "Epoch: 0094 | Loss Train: 0.3184 | Acc Train: 0.9571 | Loss Val: 0.6527 | Acc Val: 0.8233 | Time: 7.5652s\n",
      "Epoch: 0095 | Loss Train: 0.3735 | Acc Train: 0.9357 | Loss Val: 0.6512 | Acc Val: 0.8233 | Time: 7.5714s\n",
      "Epoch: 0096 | Loss Train: 0.3717 | Acc Train: 0.9214 | Loss Val: 0.6490 | Acc Val: 0.8233 | Time: 7.4847s\n",
      "Epoch: 0097 | Loss Train: 0.3171 | Acc Train: 0.9357 | Loss Val: 0.6470 | Acc Val: 0.8233 | Time: 7.4605s\n",
      "Epoch: 0098 | Loss Train: 0.3267 | Acc Train: 0.9500 | Loss Val: 0.6451 | Acc Val: 0.8233 | Time: 7.4933s\n",
      "Epoch: 0099 | Loss Train: 0.4020 | Acc Train: 0.9143 | Loss Val: 0.6427 | Acc Val: 0.8200 | Time: 7.4477s\n",
      "Epoch: 0100 | Loss Train: 0.3702 | Acc Train: 0.9214 | Loss Val: 0.6409 | Acc Val: 0.8167 | Time: 7.4717s\n",
      "Epoch: 0101 | Loss Train: 0.3104 | Acc Train: 0.9429 | Loss Val: 0.6387 | Acc Val: 0.8167 | Time: 7.4275s\n",
      "Epoch: 0102 | Loss Train: 0.3267 | Acc Train: 0.9429 | Loss Val: 0.6364 | Acc Val: 0.8167 | Time: 7.4680s\n",
      "Epoch: 0103 | Loss Train: 0.4770 | Acc Train: 0.9000 | Loss Val: 0.6343 | Acc Val: 0.8167 | Time: 7.4187s\n",
      "Epoch: 0104 | Loss Train: 0.4104 | Acc Train: 0.9286 | Loss Val: 0.6329 | Acc Val: 0.8133 | Time: 7.4216s\n",
      "Epoch: 0105 | Loss Train: 0.3012 | Acc Train: 0.9357 | Loss Val: 0.6318 | Acc Val: 0.8167 | Time: 7.4455s\n",
      "Epoch: 0106 | Loss Train: 0.4112 | Acc Train: 0.9214 | Loss Val: 0.6300 | Acc Val: 0.8167 | Time: 7.4466s\n",
      "Epoch: 0107 | Loss Train: 0.3631 | Acc Train: 0.9143 | Loss Val: 0.6284 | Acc Val: 0.8167 | Time: 7.4658s\n",
      "Epoch: 0108 | Loss Train: 0.2923 | Acc Train: 0.9357 | Loss Val: 0.6272 | Acc Val: 0.8167 | Time: 7.4566s\n",
      "Epoch: 0109 | Loss Train: 0.4073 | Acc Train: 0.9357 | Loss Val: 0.6268 | Acc Val: 0.8167 | Time: 7.4486s\n",
      "Epoch: 0110 | Loss Train: 0.2987 | Acc Train: 0.9500 | Loss Val: 0.6265 | Acc Val: 0.8167 | Time: 7.4682s\n",
      "Epoch: 0111 | Loss Train: 0.3123 | Acc Train: 0.9571 | Loss Val: 0.6262 | Acc Val: 0.8167 | Time: 7.4793s\n",
      "Epoch: 0112 | Loss Train: 0.2820 | Acc Train: 0.9429 | Loss Val: 0.6256 | Acc Val: 0.8200 | Time: 7.4421s\n",
      "Epoch: 0113 | Loss Train: 0.3587 | Acc Train: 0.9357 | Loss Val: 0.6250 | Acc Val: 0.8167 | Time: 7.4678s\n",
      "Epoch: 0114 | Loss Train: 0.2964 | Acc Train: 0.9500 | Loss Val: 0.6246 | Acc Val: 0.8167 | Time: 7.4198s\n",
      "Epoch: 0115 | Loss Train: 0.2777 | Acc Train: 0.9286 | Loss Val: 0.6239 | Acc Val: 0.8133 | Time: 7.4694s\n",
      "Epoch: 0116 | Loss Train: 0.3248 | Acc Train: 0.9286 | Loss Val: 0.6224 | Acc Val: 0.8133 | Time: 7.4851s\n",
      "Epoch: 0117 | Loss Train: 0.3023 | Acc Train: 0.9286 | Loss Val: 0.6209 | Acc Val: 0.8133 | Time: 7.4425s\n",
      "Epoch: 0118 | Loss Train: 0.2607 | Acc Train: 0.9500 | Loss Val: 0.6192 | Acc Val: 0.8133 | Time: 7.4441s\n",
      "Epoch: 0119 | Loss Train: 0.3523 | Acc Train: 0.9071 | Loss Val: 0.6173 | Acc Val: 0.8133 | Time: 7.4782s\n",
      "Epoch: 0120 | Loss Train: 0.2512 | Acc Train: 0.9500 | Loss Val: 0.6165 | Acc Val: 0.8100 | Time: 7.4195s\n",
      "Epoch: 0121 | Loss Train: 0.4232 | Acc Train: 0.9429 | Loss Val: 0.6160 | Acc Val: 0.8100 | Time: 7.4521s\n",
      "Epoch: 0122 | Loss Train: 0.3065 | Acc Train: 0.9571 | Loss Val: 0.6154 | Acc Val: 0.8100 | Time: 7.4315s\n",
      "Epoch: 0123 | Loss Train: 0.2527 | Acc Train: 0.9500 | Loss Val: 0.6148 | Acc Val: 0.8100 | Time: 7.4374s\n",
      "Epoch: 0124 | Loss Train: 0.3340 | Acc Train: 0.9214 | Loss Val: 0.6144 | Acc Val: 0.8100 | Time: 7.4395s\n",
      "Epoch: 0125 | Loss Train: 0.2730 | Acc Train: 0.9429 | Loss Val: 0.6148 | Acc Val: 0.8133 | Time: 7.4346s\n",
      "Epoch: 0126 | Loss Train: 0.3250 | Acc Train: 0.9071 | Loss Val: 0.6152 | Acc Val: 0.8100 | Time: 7.4945s\n",
      "Epoch: 0127 | Loss Train: 0.3481 | Acc Train: 0.9286 | Loss Val: 0.6153 | Acc Val: 0.8100 | Time: 7.4373s\n",
      "Epoch: 0128 | Loss Train: 0.2894 | Acc Train: 0.9214 | Loss Val: 0.6154 | Acc Val: 0.8100 | Time: 7.4901s\n",
      "Epoch: 0129 | Loss Train: 0.2229 | Acc Train: 0.9643 | Loss Val: 0.6165 | Acc Val: 0.8133 | Time: 7.4530s\n",
      "Epoch: 0130 | Loss Train: 0.2236 | Acc Train: 0.9643 | Loss Val: 0.6174 | Acc Val: 0.8167 | Time: 7.4684s\n",
      "Epoch: 0131 | Loss Train: 0.3745 | Acc Train: 0.9286 | Loss Val: 0.6182 | Acc Val: 0.8167 | Time: 7.4508s\n",
      "Epoch: 0132 | Loss Train: 0.3319 | Acc Train: 0.9357 | Loss Val: 0.6188 | Acc Val: 0.8167 | Time: 7.4318s\n",
      "Epoch: 0133 | Loss Train: 0.3058 | Acc Train: 0.9357 | Loss Val: 0.6189 | Acc Val: 0.8200 | Time: 7.4152s\n",
      "Epoch: 0134 | Loss Train: 0.3486 | Acc Train: 0.8929 | Loss Val: 0.6187 | Acc Val: 0.8167 | Time: 7.4551s\n",
      "Epoch: 0135 | Loss Train: 0.2142 | Acc Train: 0.9786 | Loss Val: 0.6189 | Acc Val: 0.8200 | Time: 7.4285s\n",
      "Epoch: 0136 | Loss Train: 0.1983 | Acc Train: 0.9643 | Loss Val: 0.6181 | Acc Val: 0.8200 | Time: 7.4282s\n",
      "Epoch: 0137 | Loss Train: 0.2557 | Acc Train: 0.9429 | Loss Val: 0.6167 | Acc Val: 0.8200 | Time: 7.4275s\n",
      "Epoch: 0138 | Loss Train: 0.2028 | Acc Train: 0.9643 | Loss Val: 0.6158 | Acc Val: 0.8233 | Time: 7.4641s\n",
      "Epoch: 0139 | Loss Train: 0.2542 | Acc Train: 0.9571 | Loss Val: 0.6151 | Acc Val: 0.8233 | Time: 7.4834s\n",
      "Epoch: 0140 | Loss Train: 0.2992 | Acc Train: 0.9429 | Loss Val: 0.6143 | Acc Val: 0.8233 | Time: 7.4854s\n",
      "Epoch: 0141 | Loss Train: 0.2933 | Acc Train: 0.9214 | Loss Val: 0.6138 | Acc Val: 0.8233 | Time: 7.5198s\n",
      "Epoch: 0142 | Loss Train: 0.2622 | Acc Train: 0.9429 | Loss Val: 0.6137 | Acc Val: 0.8167 | Time: 7.4246s\n",
      "Epoch: 0143 | Loss Train: 0.2709 | Acc Train: 0.9286 | Loss Val: 0.6135 | Acc Val: 0.8167 | Time: 7.5236s\n",
      "Epoch: 0144 | Loss Train: 0.2341 | Acc Train: 0.9714 | Loss Val: 0.6126 | Acc Val: 0.8167 | Time: 7.4709s\n",
      "Epoch: 0145 | Loss Train: 0.2220 | Acc Train: 0.9643 | Loss Val: 0.6113 | Acc Val: 0.8167 | Time: 7.5114s\n",
      "Epoch: 0146 | Loss Train: 0.2938 | Acc Train: 0.9143 | Loss Val: 0.6100 | Acc Val: 0.8133 | Time: 7.4765s\n",
      "Epoch: 0147 | Loss Train: 0.2451 | Acc Train: 0.9286 | Loss Val: 0.6079 | Acc Val: 0.8200 | Time: 7.5092s\n",
      "Epoch: 0148 | Loss Train: 0.1807 | Acc Train: 0.9714 | Loss Val: 0.6060 | Acc Val: 0.8200 | Time: 7.4796s\n",
      "Epoch: 0149 | Loss Train: 0.2529 | Acc Train: 0.9643 | Loss Val: 0.6041 | Acc Val: 0.8167 | Time: 7.4921s\n",
      "Epoch: 0150 | Loss Train: 0.3189 | Acc Train: 0.9429 | Loss Val: 0.6020 | Acc Val: 0.8167 | Time: 7.4855s\n",
      "Epoch: 0151 | Loss Train: 0.2140 | Acc Train: 0.9571 | Loss Val: 0.6000 | Acc Val: 0.8133 | Time: 7.4880s\n",
      "Epoch: 0152 | Loss Train: 0.2119 | Acc Train: 0.9714 | Loss Val: 0.5985 | Acc Val: 0.8133 | Time: 7.4750s\n",
      "Epoch: 0153 | Loss Train: 0.2671 | Acc Train: 0.9429 | Loss Val: 0.5971 | Acc Val: 0.8133 | Time: 7.4719s\n",
      "Epoch: 0154 | Loss Train: 0.2504 | Acc Train: 0.9357 | Loss Val: 0.5962 | Acc Val: 0.8133 | Time: 7.5088s\n",
      "Epoch: 0155 | Loss Train: 0.1976 | Acc Train: 0.9643 | Loss Val: 0.5960 | Acc Val: 0.8133 | Time: 7.4813s\n",
      "Epoch: 0156 | Loss Train: 0.2335 | Acc Train: 0.9357 | Loss Val: 0.5956 | Acc Val: 0.8100 | Time: 7.5353s\n",
      "Epoch: 0157 | Loss Train: 0.2153 | Acc Train: 0.9643 | Loss Val: 0.5955 | Acc Val: 0.8100 | Time: 7.4836s\n",
      "Epoch: 0158 | Loss Train: 0.2913 | Acc Train: 0.9357 | Loss Val: 0.5963 | Acc Val: 0.8167 | Time: 7.5255s\n",
      "Epoch: 0159 | Loss Train: 0.2174 | Acc Train: 0.9786 | Loss Val: 0.5971 | Acc Val: 0.8167 | Time: 7.4791s\n",
      "Epoch: 0160 | Loss Train: 0.2160 | Acc Train: 0.9500 | Loss Val: 0.5986 | Acc Val: 0.8167 | Time: 7.5246s\n",
      "Epoch: 0161 | Loss Train: 0.2015 | Acc Train: 0.9786 | Loss Val: 0.6000 | Acc Val: 0.8167 | Time: 7.4803s\n",
      "Epoch: 0162 | Loss Train: 0.1895 | Acc Train: 0.9786 | Loss Val: 0.6016 | Acc Val: 0.8167 | Time: 7.5655s\n",
      "Epoch: 0163 | Loss Train: 0.1661 | Acc Train: 0.9714 | Loss Val: 0.6031 | Acc Val: 0.8167 | Time: 7.4934s\n",
      "Epoch: 0164 | Loss Train: 0.2708 | Acc Train: 0.9500 | Loss Val: 0.6049 | Acc Val: 0.8167 | Time: 7.5632s\n",
      "Epoch: 0165 | Loss Train: 0.2448 | Acc Train: 0.9500 | Loss Val: 0.6060 | Acc Val: 0.8167 | Time: 7.4668s\n",
      "Epoch: 0166 | Loss Train: 0.2213 | Acc Train: 0.9571 | Loss Val: 0.6072 | Acc Val: 0.8167 | Time: 7.5698s\n",
      "Epoch: 0167 | Loss Train: 0.2236 | Acc Train: 0.9571 | Loss Val: 0.6078 | Acc Val: 0.8167 | Time: 7.5042s\n",
      "Epoch: 0168 | Loss Train: 0.2010 | Acc Train: 0.9643 | Loss Val: 0.6086 | Acc Val: 0.8167 | Time: 7.5067s\n",
      "Epoch: 0169 | Loss Train: 0.2504 | Acc Train: 0.9429 | Loss Val: 0.6095 | Acc Val: 0.8167 | Time: 7.5080s\n",
      "Epoch: 0170 | Loss Train: 0.1953 | Acc Train: 0.9714 | Loss Val: 0.6103 | Acc Val: 0.8167 | Time: 7.5072s\n",
      "Epoch: 0171 | Loss Train: 0.2443 | Acc Train: 0.9714 | Loss Val: 0.6112 | Acc Val: 0.8167 | Time: 7.5327s\n",
      "Epoch: 0172 | Loss Train: 0.1932 | Acc Train: 0.9500 | Loss Val: 0.6123 | Acc Val: 0.8167 | Time: 7.5154s\n",
      "Epoch: 0173 | Loss Train: 0.3020 | Acc Train: 0.9286 | Loss Val: 0.6131 | Acc Val: 0.8167 | Time: 7.5662s\n",
      "Epoch: 0174 | Loss Train: 0.2042 | Acc Train: 0.9643 | Loss Val: 0.6135 | Acc Val: 0.8133 | Time: 7.5109s\n",
      "Epoch: 0175 | Loss Train: 0.2405 | Acc Train: 0.9429 | Loss Val: 0.6142 | Acc Val: 0.8133 | Time: 7.5611s\n",
      "Epoch: 0176 | Loss Train: 0.2301 | Acc Train: 0.9429 | Loss Val: 0.6148 | Acc Val: 0.8100 | Time: 7.5152s\n",
      "Epoch: 0177 | Loss Train: 0.2581 | Acc Train: 0.9643 | Loss Val: 0.6145 | Acc Val: 0.8100 | Time: 7.5454s\n",
      "Epoch: 0178 | Loss Train: 0.2515 | Acc Train: 0.9357 | Loss Val: 0.6142 | Acc Val: 0.8100 | Time: 7.5109s\n",
      "Epoch: 0179 | Loss Train: 0.1934 | Acc Train: 0.9571 | Loss Val: 0.6132 | Acc Val: 0.8100 | Time: 7.5442s\n",
      "Epoch: 0180 | Loss Train: 0.2172 | Acc Train: 0.9571 | Loss Val: 0.6124 | Acc Val: 0.8100 | Time: 7.5113s\n",
      "Epoch: 0181 | Loss Train: 0.2358 | Acc Train: 0.9500 | Loss Val: 0.6119 | Acc Val: 0.8133 | Time: 7.5439s\n",
      "Epoch: 0182 | Loss Train: 0.1533 | Acc Train: 0.9786 | Loss Val: 0.6112 | Acc Val: 0.8133 | Time: 7.5036s\n",
      "Epoch: 0183 | Loss Train: 0.2359 | Acc Train: 0.9357 | Loss Val: 0.6104 | Acc Val: 0.8133 | Time: 7.5618s\n",
      "Epoch: 0184 | Loss Train: 0.2235 | Acc Train: 0.9429 | Loss Val: 0.6080 | Acc Val: 0.8167 | Time: 7.4987s\n",
      "Epoch: 0185 | Loss Train: 0.1648 | Acc Train: 0.9786 | Loss Val: 0.6060 | Acc Val: 0.8167 | Time: 7.5306s\n",
      "Epoch: 0186 | Loss Train: 0.1779 | Acc Train: 0.9786 | Loss Val: 0.6049 | Acc Val: 0.8200 | Time: 7.5202s\n",
      "Epoch: 0187 | Loss Train: 0.2280 | Acc Train: 0.9571 | Loss Val: 0.6038 | Acc Val: 0.8200 | Time: 7.5271s\n",
      "Epoch: 0188 | Loss Train: 0.1849 | Acc Train: 0.9786 | Loss Val: 0.6029 | Acc Val: 0.8200 | Time: 7.5420s\n",
      "Epoch: 0189 | Loss Train: 0.1870 | Acc Train: 0.9643 | Loss Val: 0.6018 | Acc Val: 0.8200 | Time: 7.5071s\n",
      "Epoch: 0190 | Loss Train: 0.2128 | Acc Train: 0.9643 | Loss Val: 0.6012 | Acc Val: 0.8200 | Time: 7.5644s\n",
      "Epoch: 0191 | Loss Train: 0.1628 | Acc Train: 0.9714 | Loss Val: 0.6009 | Acc Val: 0.8200 | Time: 7.5127s\n",
      "Epoch: 0192 | Loss Train: 0.2085 | Acc Train: 0.9357 | Loss Val: 0.6007 | Acc Val: 0.8200 | Time: 7.5487s\n",
      "Epoch: 0193 | Loss Train: 0.1703 | Acc Train: 0.9786 | Loss Val: 0.6011 | Acc Val: 0.8200 | Time: 7.5100s\n",
      "Epoch: 0194 | Loss Train: 0.1859 | Acc Train: 0.9643 | Loss Val: 0.6020 | Acc Val: 0.8167 | Time: 7.5378s\n",
      "Epoch: 0195 | Loss Train: 0.1885 | Acc Train: 0.9857 | Loss Val: 0.6032 | Acc Val: 0.8100 | Time: 7.5248s\n",
      "Epoch: 0196 | Loss Train: 0.2217 | Acc Train: 0.9571 | Loss Val: 0.6050 | Acc Val: 0.8100 | Time: 7.5446s\n",
      "Epoch: 0197 | Loss Train: 0.2357 | Acc Train: 0.9571 | Loss Val: 0.6074 | Acc Val: 0.8100 | Time: 7.5045s\n",
      "Epoch: 0198 | Loss Train: 0.1425 | Acc Train: 0.9786 | Loss Val: 0.6089 | Acc Val: 0.8067 | Time: 7.5447s\n",
      "Epoch: 0199 | Loss Train: 0.2023 | Acc Train: 0.9643 | Loss Val: 0.6110 | Acc Val: 0.8067 | Time: 7.5164s\n",
      "Epoch: 0200 | Loss Train: 0.2339 | Acc Train: 0.9500 | Loss Val: 0.6133 | Acc Val: 0.8067 | Time: 7.5426s\n",
      "Training finished. Total time: 1504.3343s\n",
      "Loading best custom model weights...\n",
      "\n",
      ">>> Custom Model Test Results: loss= 0.6171, accuracy= 0.8200\n"
     ]
    }
   ],
   "source": [
    "# --- TRAIN THE CUSTOM MODEL ---\n",
    "\n",
    "# Re-initialize model with the new class\n",
    "model_custom = GAT_With_HeadAttention(nfeat=features.shape[1], \n",
    "                                      nhid=8, \n",
    "                                      nclass=labels.max().item() + 1, \n",
    "                                      dropout=0.6, \n",
    "                                      nheads=8, \n",
    "                                      alpha=0.2)\n",
    "\n",
    "model_custom.to(device)\n",
    "optimizer_custom = optim.Adam(model_custom.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "print(f\"Custom Model Architecture:\\n{model_custom}\")\n",
    "\n",
    "def train_custom(epoch):\n",
    "    t = time.time()\n",
    "    model_custom.train()\n",
    "    optimizer_custom.zero_grad()\n",
    "    \n",
    "    output = model_custom(features, adj)\n",
    "    \n",
    "    loss_train = criterion(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    \n",
    "    loss_train.backward()\n",
    "    optimizer_custom.step()\n",
    "    \n",
    "    model_custom.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model_custom(features, adj)\n",
    "        loss_val = criterion(output[idx_val], labels[idx_val])\n",
    "        acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "        \n",
    "    print(f'Epoch: {epoch+1:04d} | '\n",
    "          f'Loss Train: {loss_train.item():.4f} | '\n",
    "          f'Acc Train: {acc_train.item():.4f} | '\n",
    "          f'Loss Val: {loss_val.item():.4f} | '\n",
    "          f'Acc Val: {acc_val.item():.4f} | '\n",
    "          f'Time: {time.time() - t:.4f}s')\n",
    "          \n",
    "    return loss_val.item()\n",
    "\n",
    "# Main Loop for Custom Model\n",
    "print(\"\\nStarting training for Custom Model...\")\n",
    "best_val_loss_custom = float('inf')\n",
    "patience_counter = 0\n",
    "start_total = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    val_loss = train_custom(epoch)\n",
    "\n",
    "    if val_loss < best_val_loss_custom:\n",
    "        best_val_loss_custom = val_loss\n",
    "        torch.save(model_custom.state_dict(), 'best_gat_custom.pkl')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(f\"Training finished. Total time: {time.time() - start_total:.4f}s\")\n",
    "\n",
    "# Test Custom Model\n",
    "print(\"Loading best custom model weights...\")\n",
    "model_custom.load_state_dict(torch.load('best_gat_custom.pkl'))\n",
    "\n",
    "model_custom.eval()\n",
    "with torch.no_grad():\n",
    "    output = model_custom(features, adj)\n",
    "    loss_test = criterion(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "\n",
    "print(f\"\\n>>> Custom Model Test Results: \"\n",
    "      f\"loss= {loss_test.item():.4f}, \"\n",
    "      f\"accuracy= {acc_test.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cc0f6b-3649-49f4-9a94-88ce976fe933",
   "metadata": {},
   "source": [
    "Report: Graph Attention Networks (GAT) Implementation & Enhancement1. Introduction & ObjectiveIn this project, we aimed to reproduce the results of the Graph Attention Networks (GAT) paper (Velikovi et al., ICLR 2018) and propose a novel architectural improvement. We utilized the Cora citation network dataset to evaluate both the baseline implementation and our proposed \"Dynamic Head Attention\" mechanism.2. MethodologyA. Baseline Model (Standard GAT)We implemented the standard GAT architecture from scratch using PyTorch. The model consists of two main components:Graph Attention Layer: Computes attention coefficients $\\alpha_{ij}$ between a node and its neighbors using a shared linear transformation and a LeakyReLU non-linearity.$$\\alpha_{ij} = \\text{softmax}(\\text{LeakyReLU}(\\vec{a}^T [W\\vec{h}_i || W\\vec{h}_j]))$$Multi-Head Attention: We employed 8 attention heads in the first layer (concatenated outputs) and 1 attention head in the output layer (classification), following the paper's configuration for the Cora dataset.B. Novel Contribution: Dynamic Head AttentionHypothesis: In the standard GAT, the output layer aggregates information from multiple heads (if used) or relies on a single head. We hypothesized that not all attention heads contribute equally to the classification of a specific node.Proposed Solution: We introduced a \"Head Attention\" mechanism at the output layer.Instead of simply averaging the outputs of $K$ heads, we use a learnable linear projection to calculate an \"importance score\" ($\\beta_k$) for each head.The final node representation is a weighted sum of the heads, where weights are learned dynamically per node.$$\\vec{h}_{final} = \\sum_{k=1}^{K} \\text{softmax}(\\beta_k) \\cdot \\vec{h}'_k$$3. Implementation DetailsFramework: PyTorchDataset: Cora (2708 nodes, 5429 edges, 7 classes).Preprocessing: Graph symmetrization ($A \\to A + A^T$) and self-loop inclusion.Hyperparameters:Learning Rate: 0.005Dropout: 0.6Weight Decay: 5e-4Epochs: 200 (with Early Stopping)4. Experimental ResultsBoth models were trained under identical conditions. The performance was evaluated on a held-out test set of 1000 nodes.Model ArchitectureTest AccuracyTest LossTraining TimeBaseline GAT (Paper Reproduction)82.10%0.5904~40sEnhanced GAT (Dynamic Head Attn)82.00%0.6171~1500s**Note: The increased training time in the Enhanced GAT is due to the non-vectorized implementation of the custom aggregation loop, which can be optimized in future work.5. Discussion & Critical AnalysisOur experiments yielded two key insights:Reproduction Success: The Baseline GAT achieved 82.1% accuracy, closely matching the original paper's reported accuracy (~83.0%). This validates the correctness of our implementation of the core attention mechanism and masking strategies.Analysis of the Contribution: The proposed \"Dynamic Head Attention\" did not significantly outperform the baseline (82.0% vs 82.1%).Over-parameterization: The Cora dataset is relatively small (140 training nodes). Adding a secondary attention mechanism increased the model complexity, likely causing the model to learn noise or struggle with convergence compared to the simpler averaging method.Occam's Razor: For homophilous citation graphs like Cora, simple aggregation strategies often suffice. The dynamic mechanism might prove more effective on larger, more heterogeneous datasets where head specialization is more critical.6. ConclusionWe successfully implemented a functional Graph Attention Network that achieves state-of-the-art performance on the Cora dataset. While our architectural enhancement did not yield an accuracy boost on this specific dataset, the implementation demonstrates the flexibility of GATs and opens avenues for testing on more complex graph tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947a5e5e-da56-4ae4-816d-953947646051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
